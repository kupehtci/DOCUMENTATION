<<<<<<< Updated upstream
#ADVANCED_COMMUNICATIONS_SYSTEM

## Entropy concepts

The entropy is the quantity of information without redundancy that a source has. 

In computer science, its the minimum bits quantity used to codify an analogical information without losing information. 

This indicates that any value under this one, imply the loss of data. 

Also measures the randomness of the information. 

### Calculations

A message or the quantity of the data that can be coded in binary (1 and 0): 

* x = quantity of data
* with `n` bits we can codify $2^n$ different values. 

=======
#ADVANCED_COMMUNICATIONS_SYSTEM

## Entropy concepts

The entropy is the quantity of information without redundancy that a source has. 

In computer science, its the minimum bits quantity used to codify an analogical information without losing information. 

This indicates that any value under this one, imply the loss of data. 

Also measures the randomness of the information. 

### Calculations

A message or the quantity of the data that can be coded in binary (1 and 0): 

* x = quantity of data
* with `n` bits we can codify $2^n$ different values. 
* so we can calculate \[n\] number of bits to transmit a signal with x number of possible values. 
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
$$x=2^n  \ \ \ \Â \ n = log_2(x)$$ If each unit of data has an equal probability of appearing in a 